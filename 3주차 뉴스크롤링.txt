import pdb
import json
import datetime as dt
import boto3
import requests

from bs4 import BeautifulSoup #우리가 원하는 component를 쉽게 parsing할 수 있도록함
from urllib.parse import urlparse
from dateutil.relativedelta import relativedelta

def fetch_news_list(datestr, page):
    print(f'Fetching page {page}')

    url = 'https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101&date={}&page={}'.format(
        datestr, page
    )

    #print(url)

    #crawler가 아니라 사람이라고 속이는 과정(크롤러인줄 알면 사이트에서 못하게 막음)
    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.8'}
    resp = requests.get(url, headers=headers)
    # print(resp.status_code)
    # print(resp.text) #크롬창에서 f12 눌렀을 때 보이는 화면을 구성하는 코드를 그대로 들고오는것

    soup = BeautifulSoup(resp.text, 'html.parser')
    #print(soup.title)

    list_body = soup.find("div", {"class": "list_body"})

    buffer = []

    for item in list_body.find_all("li"):
       # print(item)
       link = item.find_all("a")[-1]
       title = link.text.strip()

       parsed_url = urlparse(link['href'])

       msg_id = parsed_url.path.split('/')
       msg_id = '-'.join(msg_id[-2:])

       # print(title)
       # print(msg_id)
       
       body = {
           'msg_id': msg_id,
           'title': title,
           'url': link['href'],
       }

       entry = {
           'Id': msg_id,
           'MessageBody': json.dumps(body),
       }

       # print(entry)

       buffer.append(entry)

    return buffer
        #pdb.set_trace()
    # pass
# 위 headers 넣고 돌렸을 때 200이 뜨면 성공했다는 뜻

def fetch_news_list_for_date(queue, date):
    datestr = date.strftime('%Y%m%d') #date를 문자열로 바꿔주는 것

    print('[{}] Fetching news list on {}...'.format(
        dt.datetime.now(), datestr
    ))

    buffer=[]
    
    for page in range(1, 10):
        entries = fetch_news_list(datestr, page)

        if len(entries)==0:
            break

        if len(buffer)>0 and entries[-1]['Id'] == buffer[-1]['Id']:
            break

        buffer += entries

        if len(entries)<20:
            break
    print('Total number of Fetched articles: {}'.format(len(buffer)))

# 이 윗부분까지가 뉴스를 가져오는거고, 밑부분부터 SQS에 기사를 넣는 작업
    temp = { x['Id']: x for x in buffer} #중복을 해결하는 방법
    #이 한 줄이 아래 3줄과 같은 얘기임. 줄여쓴것.
    # temp = {}
    # for x in buffer:
    #     temp[x['Id']] = x
    buffer = list(temp.values()) #중복을 해결한고 다시 바꿔주는 것

    print('Pushing to AWS SQS', end='')

    if len(buffer)>0:
        for idx in range(0, len(buffer), 10):
            print('.', end='', flush=True)

            chunk = buffer[idx:idx+10]
            queue.send_messages(Entries=chunk)

        print()
        print('Successfully pushed to AWS')

    pdb.set_trace()



if __name__=="__main__":
    sqs = boto3.resource('sqs')
    queue = sqs.get_queue_by_name(QueueName='naver-news-list')


    base_date = dt.datetime(2022,4,1)
    

    for days in range(30):
        date = base_date + relativedelta(days=days)

        fetch_news_list_for_date(queue, date)

